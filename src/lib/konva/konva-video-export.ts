import Konva from 'konva'
import type { Layer, DesignData } from '@/types/template'
import type { AudioConfig } from '@/components/audio/audio-selection-modal'

export interface VideoExportOptions {
  fps?: number // Frames por segundo (padr√£o: 30)
  duration?: number // Dura√ß√£o em segundos
  format?: 'webm' | 'mp4' // Formato de sa√≠da
  quality?: number // Qualidade (0-1)
  audioConfig?: AudioConfig // Configura√ß√£o de √°udio (padr√£o: √°udio original)
}

export interface VideoExportProgress {
  phase: 'preparing' | 'recording' | 'finalizing' | 'converting' | 'uploading' | 'queued'
  progress: number // 0-100
}

interface StageCleanupState {
  previousSelection: string[]
  previousZoom: number
  previousPosition: { x: number; y: number }
  guidesWasVisible: boolean
  invisibleLayersState: Array<{
    node: Konva.Node
    originalOpacity: number
    originalVisible: boolean
  }>
}

/**
 * Prepara o stage para exporta√ß√£o limpa (sem guides, transformers, etc)
 * Baseado na fun√ß√£o exportDesign do template-editor-context.tsx
 */
async function prepareStageForExport(
  stage: Konva.Stage,
  design: DesignData,
  setSelectedLayerIds: (ids: string[]) => void,
  selectedLayerIdsRef: React.MutableRefObject<string[]>,
  zoom: number,
  setZoomState: (zoom: number) => void
): Promise<StageCleanupState> {
  // Salvar estado atual
  const previousSelection = [...selectedLayerIdsRef.current]
  const previousZoom = zoom
  const previousPosition = { x: stage.x(), y: stage.y() }

  const invisibleLayersState: Array<{
    node: Konva.Node
    originalOpacity: number
    originalVisible: boolean
  }> = []

  // 1. Limpar sele√ß√£o para ocultar transformers
  setSelectedLayerIds([])

  // 2. Aguardar pr√≥ximo frame para React atualizar
  await new Promise((resolve) => requestAnimationFrame(resolve))

  // 3. Normalizar zoom para 100% (escala 1:1)
  setZoomState(1)
  stage.scale({ x: 1, y: 1 })
  stage.position({ x: 0, y: 0 })

  // 4. Aguardar frame para zoom ser aplicado
  await new Promise((resolve) => requestAnimationFrame(resolve))

  // 5. Ocultar camada de guides temporariamente
  const guidesLayer = stage.findOne('.guides-layer')
  const guidesWasVisible = guidesLayer?.visible() ?? false
  if (guidesLayer) {
    guidesLayer.visible(false)
  }

  // 6. Ocultar completamente camadas invis√≠veis (visible: false)
  const contentLayer = stage.findOne('.content-layer') as Konva.Layer | undefined

  if (contentLayer) {
    const children = (contentLayer as Konva.Layer).getChildren()

    children.forEach((node: Konva.Node) => {
      const layerId = node.id()
      const layer = design.layers.find((l) => l.id === layerId)

      // Se a camada est√° marcada como invis√≠vel, ocultar completamente para exporta√ß√£o
      if (layer && layer.visible === false) {
        // Salvar estado original
        invisibleLayersState.push({
          node,
          originalOpacity: node.opacity(),
          originalVisible: node.visible(),
        })

        // Ocultar node do Konva
        node.visible(false)
      }
    })
  }

  // 7. For√ßar redraw para aplicar mudan√ßas
  stage.batchDraw()

  // 8. Aguardar frame para garantir que mudan√ßas foram aplicadas
  await new Promise((resolve) => requestAnimationFrame(resolve))

  return {
    previousSelection,
    previousZoom,
    previousPosition,
    guidesWasVisible,
    invisibleLayersState,
  }
}

/**
 * Restaura o stage ao estado original ap√≥s exporta√ß√£o
 */
function restoreStageState(
  stage: Konva.Stage,
  cleanupState: StageCleanupState,
  setSelectedLayerIds: (ids: string[]) => void,
  setZoomState: (zoom: number) => void
): void {
  const { previousSelection, previousZoom, previousPosition, guidesWasVisible, invisibleLayersState } =
    cleanupState

  // Restaurar estado das camadas invis√≠veis PRIMEIRO
  invisibleLayersState.forEach(({ node, originalOpacity, originalVisible }) => {
    node.opacity(originalOpacity)
    node.visible(originalVisible)
  })

  // Restaurar visibilidade dos guides
  const guidesLayer = stage.findOne('.guides-layer')
  if (guidesLayer) {
    guidesLayer.visible(guidesWasVisible)
  }

  // Restaurar zoom, posi√ß√£o e sele√ß√£o original
  setZoomState(previousZoom)
  stage.scale({ x: previousZoom, y: previousZoom })
  stage.position(previousPosition)
  stage.batchDraw()
  setSelectedLayerIds(previousSelection)
}

/**
 * Carrega √°udio de uma URL e retorna o ArrayBuffer
 */
async function loadAudioFromUrl(url: string): Promise<ArrayBuffer> {
  const response = await fetch(url)
  if (!response.ok) {
    throw new Error(`Falha ao carregar √°udio: ${response.statusText}`)
  }
  return response.arrayBuffer()
}

/**
 * Cria um AudioBuffer processado com volume, fade e trim
 */
async function createProcessedAudioBuffer(
  audioContext: AudioContext,
  arrayBuffer: ArrayBuffer,
  audioConfig: AudioConfig,
  videoDuration: number
): Promise<AudioBuffer> {
  // Decodificar √°udio
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)

  // Calcular dura√ß√£o do trecho selecionado
  const { startTime, endTime, volume, fadeIn, fadeOut, fadeInDuration, fadeOutDuration } = audioConfig
  const selectedDuration = endTime - startTime
  const outputDuration = Math.min(selectedDuration, videoDuration)

  // Criar novo buffer para o √°udio processado
  const processedBuffer = audioContext.createBuffer(
    audioBuffer.numberOfChannels,
    Math.ceil(outputDuration * audioBuffer.sampleRate),
    audioBuffer.sampleRate
  )

  // Copiar e processar cada canal
  for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
    const inputData = audioBuffer.getChannelData(channel)
    const outputData = processedBuffer.getChannelData(channel)

    // √çndices de in√≠cio e fim no buffer original
    const startSample = Math.floor(startTime * audioBuffer.sampleRate)
    const samplesToProcess = outputData.length

    for (let i = 0; i < samplesToProcess; i++) {
      const inputIndex = startSample + i
      if (inputIndex >= inputData.length) {
        outputData[i] = 0 // Sil√™ncio se ultrapassar o √°udio original
        continue
      }

      let sample = inputData[inputIndex]

      // Aplicar volume (0-100 para 0-1)
      sample *= volume / 100

      // Aplicar fade in
      if (fadeIn && i < fadeInDuration * audioBuffer.sampleRate) {
        const fadeProgress = i / (fadeInDuration * audioBuffer.sampleRate)
        sample *= fadeProgress
      }

      // Aplicar fade out
      if (fadeOut && i > samplesToProcess - fadeOutDuration * audioBuffer.sampleRate) {
        const fadeProgress =
          (samplesToProcess - i) / (fadeOutDuration * audioBuffer.sampleRate)
        sample *= fadeProgress
      }

      outputData[i] = sample
    }
  }

  return processedBuffer
}

/**
 * Cria um MediaStream com √°udio processado
 */
function createAudioStreamFromBuffer(
  audioContext: AudioContext,
  audioBuffer: AudioBuffer
): MediaStreamAudioDestinationNode {
  const source = audioContext.createBufferSource()
  source.buffer = audioBuffer

  const destination = audioContext.createMediaStreamDestination()
  source.connect(destination)

  // Iniciar reprodu√ß√£o
  source.start(0)

  return destination
}

/**
 * Cria uma track de √°udio silenciosa
 */
function createSilentAudioTrack(audioContext: AudioContext): MediaStreamTrack {
  const oscillator = audioContext.createOscillator()
  const gainNode = audioContext.createGain()
  const destination = audioContext.createMediaStreamDestination()

  // Volume zero = sil√™ncio
  gainNode.gain.value = 0

  oscillator.connect(gainNode)
  gainNode.connect(destination)
  oscillator.start()

  return destination.stream.getAudioTracks()[0]
}

/**
 * Exporta o v√≠deo com todas as layers sobrepostas usando MediaRecorder API
 * IMPORTANTE: Requer fun√ß√µes do React Context para limpar sele√ß√£o e zoom
 *
 * @param stage - Konva Stage contendo o v√≠deo e layers
 * @param videoLayer - Layer do v√≠deo base
 * @param design - Design data com layers e canvas
 * @param contextFunctions - Fun√ß√µes do template editor context
 * @param options - Op√ß√µes de exporta√ß√£o
 * @returns Blob do v√≠deo exportado
 */
export async function exportVideoWithLayers(
  stage: Konva.Stage,
  videoLayer: Layer,
  design: DesignData,
  contextFunctions: {
    setSelectedLayerIds: (ids: string[]) => void
    selectedLayerIdsRef: React.MutableRefObject<string[]>
    zoom: number
    setZoomState: (zoom: number) => void
  },
  options: VideoExportOptions = {},
  onProgress?: (progress: VideoExportProgress) => void
): Promise<Blob> {
  const {
    fps: requestedFps = 30,
    duration,
    format = 'webm',
    quality: requestedQuality = 0.8,
    audioConfig,
  } = options
  const normalizedQuality = Math.min(Math.max(requestedQuality, 0.5), 1)
  const captureFps = Math.min(60, Math.max(24, Math.round(requestedFps)))
  const audioSource = audioConfig?.source || 'original'

  onProgress?.({ phase: 'preparing', progress: 0 })

  // Verificar se o stage est√° dispon√≠vel
  if (!stage) {
    throw new Error('Stage n√£o dispon√≠vel para exporta√ß√£o')
  }

  let cleanupState: StageCleanupState | null = null

  let baseVideoOriginalMuted = false
  let baseVideoOriginalVolume = 1

  try {
    // Preparar stage (remover guides, transformers, normalizar zoom)
    cleanupState = await prepareStageForExport(
      stage,
      design,
      contextFunctions.setSelectedLayerIds,
      contextFunctions.selectedLayerIdsRef,
      contextFunctions.zoom,
      contextFunctions.setZoomState
    )

    onProgress?.({ phase: 'preparing', progress: 20 })

    // Obter o v√≠deo element
    const videoNode = stage.findOne(`#${videoLayer.id}`) as Konva.Image | null
    if (!videoNode) {
      throw new Error('VideoNode n√£o encontrado no stage')
    }

    const videoElement = videoNode.image() as HTMLVideoElement
    if (!videoElement) {
      throw new Error('Elemento de v√≠deo n√£o encontrado')
    }
    baseVideoOriginalMuted = videoElement.muted
    baseVideoOriginalVolume = typeof videoElement.volume === 'number' ? videoElement.volume : 1
    const shouldMuteBaseVideo = audioSource !== 'original'

    if (shouldMuteBaseVideo) {
      try {
        videoElement.muted = true
        videoElement.volume = 0
      } catch (error) {
        console.warn('[Video Export] N√£o foi poss√≠vel mutar o √°udio original:', error)
      }
    }

    // Calcular dura√ß√£o do v√≠deo
    // Se usar m√∫sica da biblioteca, usar dura√ß√£o do trecho selecionado
    // Caso contr√°rio, usar dura√ß√£o total do v√≠deo
    let videoDuration: number
    if (audioConfig?.source === 'library' && audioConfig.startTime !== undefined && audioConfig.endTime !== undefined) {
      videoDuration = audioConfig.endTime - audioConfig.startTime
      console.log(`[Video Export] Usando dura√ß√£o do trecho selecionado da m√∫sica: ${videoDuration}s (${audioConfig.startTime}s - ${audioConfig.endTime}s)`)
    } else {
      videoDuration = duration || videoLayer.videoMetadata?.duration || videoElement.duration || 10
      console.log(`[Video Export] Usando dura√ß√£o total do v√≠deo: ${videoDuration}s`)
    }

    // Posicionar v√≠deo no in√≠cio e garantir que o primeiro frame est√° renderizado
    videoElement.currentTime = 0
    await new Promise<void>((resolve) => {
      const handleSeeked = () => {
        resolve()
      }
      if (videoElement.readyState >= 2 && videoElement.currentTime === 0) {
        resolve()
      } else {
        videoElement.addEventListener('seeked', handleSeeked, { once: true })
      }
    })

    onProgress?.({ phase: 'preparing', progress: 30 })

    // IMPORTANTE: Konva usa m√∫ltiplos canvas (um por layer)
    // Precisamos criar um canvas offscreen que combina todos os layers
    const stageWidth = stage.width()
    const stageHeight = stage.height()

    // Criar canvas offscreen para composi√ß√£o
    const offscreenCanvas = document.createElement('canvas')
    offscreenCanvas.width = stageWidth
    offscreenCanvas.height = stageHeight
    const offscreenCtx = offscreenCanvas.getContext('2d', {
      alpha: false, // Sem transpar√™ncia = melhor performance
      willReadFrequently: false,
    })

    if (!offscreenCtx) {
      throw new Error('Falha ao criar contexto do canvas offscreen')
    }

    // Preencher fundo branco (ou usar cor de fundo do design) com o frame inicial
    videoNode.getLayer()?.batchDraw()
    stage.batchDraw()
    const initialSnapshot = stage.toCanvas()
    offscreenCtx.fillStyle = design.canvas.backgroundColor || '#FFFFFF'
    offscreenCtx.fillRect(0, 0, stageWidth, stageHeight)
    offscreenCtx.drawImage(initialSnapshot, 0, 0)

    // Verificar suporte a MediaRecorder (sempre WebM)
    const mimeType = 'video/webm;codecs=vp9'

    // Fallback para vp8 se vp9 n√£o for suportado
    const finalMimeType = MediaRecorder.isTypeSupported(mimeType)
      ? mimeType
      : 'video/webm;codecs=vp8'

    if (!MediaRecorder.isTypeSupported(finalMimeType)) {
      throw new Error(
        'Seu navegador n√£o suporta grava√ß√£o de v√≠deo. Por favor, use Chrome, Firefox ou Edge.'
      )
    }

    onProgress?.({ phase: 'preparing', progress: 40 })

    // Criar stream do canvas offscreen com frameRate fixo
    const canvasStream = offscreenCanvas.captureStream(captureFps)
    console.log(`[Video Export] Canvas stream criado com ${captureFps} FPS`)

    const primaryCanvasTrack = canvasStream.getVideoTracks()[0]
    if (primaryCanvasTrack) {
      if ('contentHint' in primaryCanvasTrack) {
        try {
          ;(primaryCanvasTrack as MediaStreamTrack & { contentHint?: string }).contentHint = 'motion'
        } catch {
          // Alguns navegadores n√£o permitem definir o hint
        }
      }
      if (typeof primaryCanvasTrack.applyConstraints === 'function') {
        try {
          await primaryCanvasTrack.applyConstraints({ frameRate: captureFps })
          console.log(`[Video Export] ‚úÖ FrameRate constraint aplicado: ${captureFps} FPS`)
        } catch (error) {
          console.warn('[Video Export] ‚ö†Ô∏è N√£o foi poss√≠vel aplicar frameRate no track:', error)
        }
      }
    }

    // Processar √°udio de acordo com audioConfig
    let stream = canvasStream
    try {
      const audioContext = new AudioContext()

      if (audioSource === 'mix' && audioConfig?.musicId) {
        // Caso 4: Mixar √°udio original + m√∫sica da biblioteca
        console.log('[Video Export] Mixando √°udio original + m√∫sica da biblioteca')
        onProgress?.({ phase: 'preparing', progress: 45 })

        // 1. Capturar √°udio original do v√≠deo
        const clonedVideoForOriginal = document.createElement('video')
        clonedVideoForOriginal.src = videoElement.src
        clonedVideoForOriginal.crossOrigin = 'anonymous'
        clonedVideoForOriginal.muted = false

        await new Promise<void>((resolve, reject) => {
          clonedVideoForOriginal.addEventListener('loadedmetadata', () => resolve(), { once: true })
          clonedVideoForOriginal.addEventListener('error', () => reject(new Error('Falha ao carregar v√≠deo clone')), { once: true })
          clonedVideoForOriginal.load()
          setTimeout(() => reject(new Error('Timeout ao carregar v√≠deo clone')), 10000)
        })

        // @ts-expect-error - captureStream exists
        const originalStream = clonedVideoForOriginal.captureStream() as MediaStream
        const originalAudioTracks = originalStream.getAudioTracks()

        if (originalAudioTracks.length === 0) {
          throw new Error('V√≠deo n√£o possui √°udio original para mixar')
        }

        // 2. Carregar m√∫sica da biblioteca
        console.log('[Video Export] Carregando m√∫sica da biblioteca:', audioConfig.musicId)
        const musicResponse = await fetch(`/api/biblioteca-musicas/${audioConfig.musicId}`)
        if (!musicResponse.ok) {
          throw new Error('Falha ao buscar informa√ß√µes da m√∫sica')
        }
        const musicData = await musicResponse.json()

        // Determinar qual URL usar baseado na vers√£o selecionada (original vs instrumental)
        const audioVersion = audioConfig.audioVersion || 'original'
        let audioUrl: string

        if (audioVersion === 'instrumental') {
          if (!musicData.hasInstrumentalStem || !musicData.instrumentalUrl) {
            throw new Error('A vers√£o instrumental n√£o est√° dispon√≠vel ainda. Por favor, aguarde o processamento.')
          }
          audioUrl = musicData.instrumentalUrl
          console.log('[Video Export] Usando vers√£o instrumental:', audioUrl)
        } else {
          audioUrl = musicData.blobUrl
          console.log('[Video Export] Usando vers√£o original:', audioUrl)
        }

        const musicArrayBuffer = await loadAudioFromUrl(audioUrl)

        // 3. Processar m√∫sica (volume, fade, trim)
        const processedMusicBuffer = await createProcessedAudioBuffer(
          audioContext,
          musicArrayBuffer,
          {
            ...audioConfig,
            volume: audioConfig.volumeMusic || 60, // Volume da m√∫sica
          },
          videoDuration
        )

        // 4. Criar n√≥s de √°udio no AudioContext
        const musicSource = audioContext.createBufferSource()
        musicSource.buffer = processedMusicBuffer

        const originalSource = audioContext.createMediaStreamSource(
          new MediaStream(originalAudioTracks)
        )

        // 5. Criar gain nodes para controlar volume individual
        const originalGain = audioContext.createGain()
        originalGain.gain.value = (audioConfig.volumeOriginal || 80) / 100

        const musicGain = audioContext.createGain()
        musicGain.gain.value = (audioConfig.volumeMusic || 60) / 100

        // 6. Conectar ao destination
        const destination = audioContext.createMediaStreamDestination()

        originalSource.connect(originalGain)
        originalGain.connect(destination)

        musicSource.connect(musicGain)
        musicGain.connect(destination)

        // 7. Iniciar reprodu√ß√£o da m√∫sica
        musicSource.start(0)

        // 8. Posicionar v√≠deo clone e N√ÉO reproduzir ainda
        clonedVideoForOriginal.currentTime = 0
        console.log('[Video Export] Mix preparado - aguardando in√≠cio da grava√ß√£o...')

        // 9. Combinar stream de v√≠deo com stream de √°udio mixado
        const audioTracks = destination.stream.getAudioTracks()
        const videoTracks = canvasStream.getVideoTracks()
        stream = new MediaStream([...videoTracks, ...audioTracks])

        // Manter refer√™ncia do clone para sincroniza√ß√£o
        // @ts-expect-error - propriedade customizada
        stream._clonedVideoElement = clonedVideoForOriginal

        console.log('[Video Export] Mix de √°udio criado com sucesso')
      } else if (audioSource === 'mute') {
        // Caso 1: V√≠deo mudo - criar track silenciosa
        console.log('[Video Export] Criando v√≠deo sem √°udio (mudo)')
        const silentTrack = createSilentAudioTrack(audioContext)
        const videoTracks = canvasStream.getVideoTracks()
        stream = new MediaStream([...videoTracks, silentTrack])
      } else if (audioSource === 'library' && audioConfig?.musicId) {
        // Caso 2: M√∫sica da biblioteca
        console.log('[Video Export] Carregando m√∫sica da biblioteca:', audioConfig.musicId)
        onProgress?.({ phase: 'preparing', progress: 45 })

        // Buscar informa√ß√µes da m√∫sica
        const musicResponse = await fetch(`/api/biblioteca-musicas/${audioConfig.musicId}`)
        if (!musicResponse.ok) {
          throw new Error('Falha ao buscar informa√ß√µes da m√∫sica')
        }
        const musicData = await musicResponse.json()

        // Determinar qual URL usar baseado na vers√£o selecionada (original vs instrumental)
        const audioVersion = audioConfig.audioVersion || 'original'
        let audioUrl: string

        if (audioVersion === 'instrumental') {
          if (!musicData.hasInstrumentalStem || !musicData.instrumentalUrl) {
            throw new Error('A vers√£o instrumental n√£o est√° dispon√≠vel ainda. Por favor, aguarde o processamento.')
          }
          audioUrl = musicData.instrumentalUrl
          console.log('[Video Export] Usando vers√£o instrumental:', audioUrl)
        } else {
          audioUrl = musicData.blobUrl
          console.log('[Video Export] Usando vers√£o original:', audioUrl)
        }

        // Carregar √°udio da biblioteca
        const musicArrayBuffer = await loadAudioFromUrl(audioUrl)

        // Processar √°udio (volume, fade, trim)
        const processedAudioBuffer = await createProcessedAudioBuffer(
          audioContext,
          musicArrayBuffer,
          audioConfig,
          videoDuration
        )

        // Criar stream de √°udio processado
        const audioDestination = createAudioStreamFromBuffer(audioContext, processedAudioBuffer)

        // Combinar stream de v√≠deo com √°udio processado
        const audioTracks = audioDestination.stream.getAudioTracks()
        const videoTracks = canvasStream.getVideoTracks()
        stream = new MediaStream([...videoTracks, ...audioTracks])

        console.log('[Video Export] M√∫sica da biblioteca adicionada com sucesso')
      } else {
        // Caso 3: √Åudio original do v√≠deo (padr√£o)
        console.log('[Video Export] Usando √°udio original do v√≠deo')
        console.log('[Video Export] URL do v√≠deo:', videoElement.src?.substring(0, 100))

        try {
          // Estrat√©gia: Criar um v√≠deo clone para capturar √°udio sem interferir no original
          const clonedVideo = document.createElement('video')
          clonedVideo.src = videoElement.src
          clonedVideo.crossOrigin = 'anonymous'
          clonedVideo.muted = false // Importante: n√£o mutar o clone

          console.log('[Video Export] Aguardando carregamento do v√≠deo clone...')

          // Aguardar v√≠deo clone carregar
          await new Promise<void>((resolve, reject) => {
            clonedVideo.addEventListener('loadedmetadata', () => {
              console.log('[Video Export] ‚úÖ V√≠deo clone carregado')
              resolve()
            }, { once: true })

            clonedVideo.addEventListener('error', (e) => {
              console.error('[Video Export] ‚ùå Erro ao carregar v√≠deo clone:', e)
              reject(new Error('Falha ao carregar v√≠deo clone'))
            }, { once: true })

            clonedVideo.load()

            // Timeout de seguran√ßa
            setTimeout(() => reject(new Error('Timeout ao carregar v√≠deo clone')), 10000)
          })

          // Tentar capturar stream do clone
          // @ts-expect-error - captureStream() existe em navegadores modernos
          const cloneStream = clonedVideo.captureStream() as MediaStream
          const audioTracks = cloneStream.getAudioTracks()

          console.log('[Video Export] Audio tracks encontradas:', audioTracks.length)

          if (audioTracks.length > 0) {
            console.log('[Video Export] ‚úÖ √Åudio original capturado com sucesso')

            // Posicionar clone no in√≠cio mas N√ÉO reproduzir ainda
            clonedVideo.currentTime = 0
            clonedVideo.muted = false
            console.log('[Video Export] Clone posicionado em 0s, aguardando in√≠cio da grava√ß√£o...')

            // Combinar stream de v√≠deo (do canvas) com √°udio (do clone)
            const videoTracks = canvasStream.getVideoTracks()
            stream = new MediaStream([...videoTracks, ...audioTracks])

            // Manter refer√™ncia do clone para sincroniza√ß√£o durante a grava√ß√£o
            // @ts-expect-error - adicionar propriedade customizada
            stream._clonedVideoElement = clonedVideo
          } else {
            console.log('[Video Export] ‚ö†Ô∏è V√≠deo n√£o possui faixas de √°udio')
            stream = canvasStream
          }
        } catch (error) {
          console.error('[Video Export] ‚ùå Erro ao capturar √°udio original:', error)
          console.log('[Video Export] Continuando sem √°udio')
          stream = canvasStream
        }
      }
    } catch (error) {
      console.warn('[Video Export] Erro ao processar √°udio:', error)
      // Continuar sem √°udio se falhar
      stream = canvasStream
    }

    // Configurar MediaRecorder
    const targetVideoBitrate = Math.round(
      Math.min(12_000_000, Math.max(6_000_000, normalizedQuality * 10_000_000))
    )
    const mediaRecorder = new MediaRecorder(stream, {
      mimeType: finalMimeType,
      videoBitsPerSecond: targetVideoBitrate,
      audioBitsPerSecond: 256_000,
    })

    const chunks: Blob[] = []

    // Coletar chunks de v√≠deo
    mediaRecorder.ondataavailable = (e) => {
      if (e.data.size > 0) {
        chunks.push(e.data)
      }
    }

    mediaRecorder.onerror = (e) => {
      console.error('[Video Export] Erro no MediaRecorder:', e)
    }

    onProgress?.({ phase: 'recording', progress: 50 })

    // Iniciar grava√ß√£o ANTES de reproduzir o v√≠deo
    mediaRecorder.start(100) // Capturar chunks a cada 100ms

    // Aguardar start com timeout
    await new Promise((resolve) => setTimeout(resolve, 200))

    // Posicionar v√≠deo no in√≠cio (novamente ap√≥s start) e reproduzir
    videoElement.currentTime = 0

    try {
      await videoElement.play()
    } catch (error) {
      const isAbortError =
        error instanceof DOMException && (error.name === 'AbortError' || error.code === DOMException.ABORT_ERR)
      if (isAbortError) {
        console.warn('[Video Export] Reprodu√ß√£o interrompida imediatamente ap√≥s play(). Prosseguindo mesmo assim.')
      } else {
        console.error('[Video Export] Erro ao reproduzir v√≠deo:', error)
        throw new Error(
          'Falha ao reproduzir v√≠deo: ' + (error instanceof Error ? error.message : 'erro desconhecido')
        )
      }
    }

    // Reproduzir v√≠deo clone simultaneamente (se existir) para capturar √°udio
    // @ts-expect-error - propriedade customizada
    const clonedVideo = stream._clonedVideoElement as HTMLVideoElement | undefined
    if (clonedVideo) {
      try {
        await clonedVideo.play()
        console.log('[Video Export] üéµ Clone iniciado - √°udio sendo capturado')
      } catch (error) {
        console.warn('[Video Export] Erro ao reproduzir clone (√°udio pode n√£o funcionar):', error)
      }
    }

    // Loop de anima√ß√£o para copiar o stage para o canvas offscreen frame por frame
    let animationId: number | null = null
    const startTime = Date.now()

    const animationLoop = () => {
      // Sincronizar tempo do clone com o v√≠deo original (se existir)
      if (clonedVideo && !clonedVideo.paused) {
        const timeDiff = Math.abs(clonedVideo.currentTime - videoElement.currentTime)
        // Se diferen√ßa > 0.1s, ressincronizar
        if (timeDiff > 0.1) {
          clonedVideo.currentTime = videoElement.currentTime
          console.log('[Video Export] üîÑ Clone ressincronizado:', videoElement.currentTime.toFixed(2), 's')
        }
      }
      // 1. For√ßar redraw do stage para atualizar com frame atual do v√≠deo
      videoNode.getLayer()?.batchDraw()
      stage.batchDraw()

      // 2. Copiar o stage renderizado para o canvas offscreen
      // Usar toCanvas() que retorna um snapshot do stage completo (todas as layers compostas)
      const stageSnapshot = stage.toCanvas()

      // 3. Limpar canvas offscreen e desenhar o snapshot
      offscreenCtx.fillStyle = design.canvas.backgroundColor || '#FFFFFF'
      offscreenCtx.fillRect(0, 0, stageWidth, stageHeight)
      offscreenCtx.drawImage(stageSnapshot, 0, 0)

      const elapsed = (Date.now() - startTime) / 1000
      if (elapsed < videoDuration) {
        animationId = requestAnimationFrame(animationLoop)
      }
    }

    // Iniciar loop de anima√ß√£o
    animationId = requestAnimationFrame(animationLoop)

    // Aguardar dura√ß√£o especificada
    await new Promise<void>((resolve) => {
      const progressInterval = setInterval(() => {
        const currentProgress = (videoElement.currentTime / videoDuration) * 100
        onProgress?.({ phase: 'recording', progress: 50 + currentProgress * 0.35 })
      }, 100)

      setTimeout(() => {
        clearInterval(progressInterval)

        // Parar loop de anima√ß√£o
        if (animationId !== null) {
          cancelAnimationFrame(animationId)
        }

        videoElement.pause()

        // Parar v√≠deo clone se existir
        // @ts-expect-error - propriedade customizada
        const clonedVideo = stream._clonedVideoElement as HTMLVideoElement | undefined
        if (clonedVideo) {
          clonedVideo.pause()
          console.log('[Video Export] V√≠deo clone pausado')
        }

        // Aguardar um pouco antes de parar para garantir que √∫ltimo frame foi capturado
        setTimeout(() => {
          mediaRecorder.stop()
          resolve()
        }, 200)
      }, videoDuration * 1000)
    })

    onProgress?.({ phase: 'finalizing', progress: 85 })

    // Aguardar finaliza√ß√£o do MediaRecorder
    await new Promise<void>((resolve) => {
      mediaRecorder.onstop = () => resolve()
    })

    // Gerar blob WebM
    const webmBlob = new Blob(chunks, { type: finalMimeType })

    // Se formato solicitado for MP4, converter usando FFmpeg.wasm
    if (format === 'mp4') {
      onProgress?.({ phase: 'converting', progress: 85 })
      console.log('[exportVideoWithLayers] Convertendo WebM para MP4...')

      try {
        const { convertWebMToMP4 } = await import('@/lib/video/ffmpeg-converter')

        const mp4Blob = await convertWebMToMP4(webmBlob, (conversionProgress) => {
          // Mapear progresso da convers√£o (0-100) para progresso total (85-100)
          const totalProgress = 85 + (conversionProgress / 100) * 15
          onProgress?.({ phase: 'converting', progress: totalProgress })
        })

        onProgress?.({ phase: 'finalizing', progress: 100 })
        return mp4Blob
      } catch (error) {
        console.error('[exportVideoWithLayers] Erro ao converter para MP4:', error)
        console.warn('[exportVideoWithLayers] Retornando WebM como fallback')
        // Fallback: retornar WebM se convers√£o falhar
        onProgress?.({ phase: 'finalizing', progress: 100 })
        return webmBlob
      }
    }

    onProgress?.({ phase: 'finalizing', progress: 100 })
    return webmBlob
  } finally {
    // Sempre restaurar estado do stage
    if (cleanupState) {
      restoreStageState(
        stage,
        cleanupState,
        contextFunctions.setSelectedLayerIds,
        contextFunctions.setZoomState
      )
    }

    try {
      const videoNode = stage.findOne(`#${videoLayer.id}`) as Konva.Image | null
      const baseVideo = videoNode?.image() as HTMLVideoElement | undefined
      if (baseVideo) {
        baseVideo.muted = baseVideoOriginalMuted
        baseVideo.volume = baseVideoOriginalVolume
      }
    } catch {
      // ignore
    }
  }
}

/**
 * Gera um thumbnail est√°tico do v√≠deo no frame atual
 *
 * @param stage - Konva Stage
 * @param videoLayer - Layer do v√≠deo
 * @returns Data URL do thumbnail
 */
export async function generateVideoThumbnail(
  stage: Konva.Stage,
  videoLayer: Layer
): Promise<string> {
  if (!stage) {
    throw new Error('Stage n√£o dispon√≠vel')
  }

  // Encontrar o VideoNode no stage
  const videoNode = stage.findOne(`#${videoLayer.id}`) as Konva.Image | null

  if (!videoNode) {
    throw new Error('VideoNode n√£o encontrado')
  }

  // Aguardar frame v√°lido do v√≠deo
  const video = videoNode.image() as HTMLVideoElement

  await new Promise<void>((resolve) => {
    if (video.readyState >= 2) {
      resolve()
    } else {
      video.addEventListener('loadeddata', () => resolve(), { once: true })
    }
  })

  // Garantir que o primeiro frame est√° renderizado antes de capturar o thumbnail
  try {
    video.currentTime = 0
  } catch (error) {
    console.warn('[generateVideoThumbnail] N√£o foi poss√≠vel definir currentTime para 0:', error)
  }

  await new Promise<void>((resolve) => {
    video.addEventListener('seeked', () => resolve(), { once: true })
    if (video.readyState >= 2 && video.currentTime === 0) {
      resolve()
    }
  })

  videoNode.getLayer()?.batchDraw()
  stage.batchDraw()

  // Gerar thumbnail do stage atual
  const dataURL = stage.toDataURL({
    pixelRatio: 1,
    mimeType: 'image/jpeg',
    quality: 0.8,
  })

  return dataURL
}

/**
 * Verifica se o navegador suporta exporta√ß√£o de v√≠deo
 */
export function checkVideoExportSupport(): {
  supported: boolean
  message?: string
} {
  if (typeof MediaRecorder === 'undefined') {
    return {
      supported: false,
      message: 'MediaRecorder API n√£o dispon√≠vel neste navegador',
    }
  }

  const webmVp9 = MediaRecorder.isTypeSupported('video/webm;codecs=vp9')
  const webmVp8 = MediaRecorder.isTypeSupported('video/webm;codecs=vp8')

  if (!webmVp9 && !webmVp8) {
    return {
      supported: false,
      message: 'Formato de v√≠deo WebM n√£o suportado',
    }
  }

  return { supported: true }
}
